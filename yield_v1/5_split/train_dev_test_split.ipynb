{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87fd9180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Imports ----------------\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import yaml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd2278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Args ----------------\n",
    "CHUNK_SIZE = 128\n",
    "SUBSET_SIZE = 0.10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9202fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Config ----------------\n",
    "with open(\"../../../config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "data_folder = os.path.join(config[\"paths\"][\"proj_store\"], \"data\")\n",
    "\n",
    "# Directories\n",
    "input_folder = f\"{data_folder}/yield_v1_base/\"\n",
    "output_folder = f\"{data_folder}/yield-v1/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "subset_label = f\"{int(SUBSET_SIZE * 100)}pct\"\n",
    "subset_output_folder = os.path.join(data_folder, f\"yield-v1-small{subset_label}\")\n",
    "os.makedirs(subset_output_folder, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb26790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index\n",
    "index_data = []\n",
    "for root, _, files in os.walk(input_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    domain = data.get('domain', 'unknown')\n",
    "                    index_data.append({'file_path': file_path, 'domain': domain})\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426b4429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 'journalistic_investigations': 129 files → train=103, dev=13, test=13\n",
      "Domain 'judicial_proceedings': 621 files → train=497, dev=62, test=62\n",
      "Domain 'academic_interviews': 148 files → train=118, dev=15, test=15\n",
      "Domain 'oral_history': 1383 files → train=1106, dev=138, test=139\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-000.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-001.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-002.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-003.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-004.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-005.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-006.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-007.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-008.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-009.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-010.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-011.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-012.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/train/train-013.json\n",
      "Wrote 32 dialogues to /data/yield-v1/data/yield-v1/train/train-014.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/dev/dev-000.json\n",
      "Wrote 100 dialogues to /data/yield-v1/data/yield-v1/dev/dev-001.json\n",
      "Wrote 128 dialogues to /data/yield-v1/data/yield-v1/test/test-000.json\n",
      "Wrote 101 dialogues to /data/yield-v1/data/yield-v1/test/test-001.json\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Stratified split\n",
    "domain_files = {}\n",
    "for entry in index_data:\n",
    "    domain_files.setdefault(entry['domain'], []).append(entry['file_path'])\n",
    "\n",
    "train_files, dev_files, test_files = [], [], []\n",
    "\n",
    "for domain, files in domain_files.items():\n",
    "    random.shuffle(files)\n",
    "    total = len(files)\n",
    "    train_count = round(total * 0.8)\n",
    "    dev_count = round(total * 0.1)\n",
    "    test_count = total - train_count - dev_count  # ensure sum\n",
    "    \n",
    "    train_files.extend(files[:train_count])\n",
    "    dev_files.extend(files[train_count:train_count + dev_count])\n",
    "    test_files.extend(files[train_count + dev_count:])\n",
    "    \n",
    "    print(f\"Domain '{domain}': {len(files)} files → train={train_count}, dev={dev_count}, test={test_count}\")\n",
    "\n",
    "splits = {\n",
    "    'train': train_files,\n",
    "    'dev': dev_files,\n",
    "    'test': test_files\n",
    "}\n",
    "\n",
    "# write aggregated files\n",
    "for split_name, file_list in splits.items():\n",
    "    split_output_folder = os.path.join(output_folder, split_name)\n",
    "    os.makedirs(split_output_folder, exist_ok=True)\n",
    "    \n",
    "    num_chunks = math.ceil(len(file_list) / CHUNK_SIZE)\n",
    "    for i in range(num_chunks):\n",
    "        chunk_files = file_list[i * CHUNK_SIZE : (i + 1) * CHUNK_SIZE]\n",
    "        dialogues_chunk = []\n",
    "        \n",
    "        for file_path in chunk_files:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    dialogues_chunk.append(json.load(f))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "        \n",
    "        output_path = os.path.join(split_output_folder, f'{split_name}-{i:03d}.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "            json.dump(dialogues_chunk, out_file, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Wrote {len(dialogues_chunk)} dialogues to {output_path}\")\n",
    "\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc11fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Domain 'journalistic_investigations': 103 → subset=10\n",
      "[train] Domain 'judicial_proceedings': 497 → subset=50\n",
      "[train] Domain 'academic_interviews': 118 → subset=12\n",
      "[train] Domain 'oral_history': 1106 → subset=111\n",
      "[dev] Domain 'journalistic_investigations': 13 → subset=1\n",
      "[dev] Domain 'judicial_proceedings': 62 → subset=6\n",
      "[dev] Domain 'academic_interviews': 15 → subset=2\n",
      "[dev] Domain 'oral_history': 138 → subset=14\n",
      "[test] Domain 'journalistic_investigations': 13 → subset=1\n",
      "[test] Domain 'judicial_proceedings': 62 → subset=6\n",
      "[test] Domain 'academic_interviews': 15 → subset=2\n",
      "[test] Domain 'oral_history': 139 → subset=14\n",
      "[subset] Wrote 128 dialogues to /data/yield-v1/data/yield-v1-small10pct/train/train-000.json\n",
      "[subset] Wrote 55 dialogues to /data/yield-v1/data/yield-v1-small10pct/train/train-001.json\n",
      "[subset] Wrote 23 dialogues to /data/yield-v1/data/yield-v1-small10pct/dev/dev-000.json\n",
      "[subset] Wrote 23 dialogues to /data/yield-v1/data/yield-v1-small10pct/test/test-000.json\n",
      "stratified subset done!\n"
     ]
    }
   ],
   "source": [
    "# Create stratified subset\n",
    "\n",
    "\n",
    "subset_splits = {}\n",
    "for split_name, file_list in splits.items():\n",
    "    subset_files = []\n",
    "    domain_files_split = {}\n",
    "\n",
    "    # Organize files by domain for the split\n",
    "    for file_path in file_list:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            domain = json.load(f).get('domain', 'unknown')\n",
    "        domain_files_split.setdefault(domain, []).append(file_path)\n",
    "\n",
    "    # Sample from each domain\n",
    "    for domain, files in domain_files_split.items():\n",
    "        k = max(1, round(len(files) * SUBSET_SIZE))  # ensure at least one\n",
    "        subset_files.extend(random.sample(files, k))\n",
    "        print(f\"[{split_name}] Domain '{domain}': {len(files)} → subset={k}\")\n",
    "\n",
    "    subset_splits[split_name] = subset_files\n",
    "\n",
    "# write aggregated files for subset\n",
    "for split_name, file_list in subset_splits.items():\n",
    "    split_output_folder = os.path.join(subset_output_folder, split_name)\n",
    "    os.makedirs(split_output_folder, exist_ok=True)\n",
    "    \n",
    "    num_chunks = math.ceil(len(file_list) / CHUNK_SIZE)\n",
    "    for i in range(num_chunks):\n",
    "        chunk_files = file_list[i * CHUNK_SIZE : (i + 1) * CHUNK_SIZE]\n",
    "        dialogues_chunk = []\n",
    "        \n",
    "        for file_path in chunk_files:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    dialogues_chunk.append(json.load(f))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "        \n",
    "        output_path = os.path.join(split_output_folder, f'{split_name}-{i:03d}.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "            json.dump(dialogues_chunk, out_file, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"[subset] Wrote {len(dialogues_chunk)} dialogues to {output_path}\")\n",
    "\n",
    "print(\"stratified subset done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yield-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
