{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Imports ----------------\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Config ----------------\n",
    "with open(\"../../../config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "data_folder = os.path.join(config[\"paths\"][\"proj_store\"], \"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Setup ----------------\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = \"/usr/bin/google-chrome\"  # Update with your Chrome path\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Automatically manage ChromeDriver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "#try:\n",
    "# Open the webpage\n",
    "driver.get('https://www.jfklibrary.org/asset-viewer/archives/rpcv')\n",
    "# Wait for content to load\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.ID, 'asset-viewer-container'))\n",
    ")\n",
    "# Get and print HTML content\n",
    "html_content = driver.page_source\n",
    "\n",
    "\n",
    "#finally:\n",
    "#    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "html_content[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract links\n",
    "\n",
    "def extract_anchor_tags(html_string):\n",
    "    soup = BeautifulSoup(html_string, 'html.parser')\n",
    "    anchor_data = []\n",
    "\n",
    "    # Extract all <a> tags with an href attribute\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        text = a_tag.get_text(strip=True)  # Get visible text of the link\n",
    "        anchor_data.append({'href': href, 'text': text})\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    return pd.DataFrame(anchor_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract anchor tags into a DataFrame\n",
    "df = extract_anchor_tags(html_content)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the DataFrame to rows where href starts with \"RPCV\"\n",
    "subset_df = df[df['href'].str.startswith(\"RPCV\")]\n",
    "\n",
    "display(subset_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which pages have PDFs\n",
    "def check_for_text_in_links_verbose(df, base_url, search_text, driver):\n",
    "    df = df.copy()\n",
    "    df['full_url'] = base_url + df['href']\n",
    "    \n",
    "    valid_entries = []  # Store only valid rows\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        url = row['full_url']\n",
    "        try:\n",
    "            #print(f\"Visiting URL: {url}\")\n",
    "            driver.get(url)  # Load page using Selenium\n",
    "\n",
    "            # Get Initial Page Content\n",
    "            page_content = driver.page_source  # Get the HTML before waiting\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "            # Check if \"Download PDF Transcript\" exists\n",
    "            if search_text not in soup.text:\n",
    "                print(f\"Skipping URL (text not found): {url}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Text found in: {url}\")\n",
    "\n",
    "            # Extract \"Download PDF Transcript\" Link Immediately\n",
    "            pdf_download_url = None\n",
    "            for link in soup.find_all(\"a\", class_=\"button\"):\n",
    "                link_text = link.get_text(strip=True)\n",
    "                href = link.get(\"href\")\n",
    "\n",
    "                if \"Download PDF Transcript\" in link_text and href:\n",
    "                    pdf_download_url = href  # Store PDF link\n",
    "\n",
    "            # Check if \"Download Audio File\" Exists\n",
    "            audio_download_url = None\n",
    "            audio_link_found = any(\n",
    "                \"Download Audio File\" in link.get_text(strip=True) for link in soup.find_all(\"a\", class_=\"button download-link\")\n",
    "            )\n",
    "\n",
    "            # Wait for `download=\"\"` to be populated (Only If Audio Exists)\n",
    "            if audio_link_found:\n",
    "                #print(f\"Waiting for the 'download' attribute on: {url}\")\n",
    "                try:\n",
    "                    # Wait until the \"Download Audio File\" `download` attribute is populated\n",
    "                    audio_element = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"a.button.download-link[download]\"))\n",
    "                    )\n",
    "                    \n",
    "                    # Re-fetch the updated HTML\n",
    "                    new_page_content = driver.page_source  # Get updated HTML\n",
    "                    new_soup = BeautifulSoup(new_page_content, 'html.parser')\n",
    "\n",
    "                    # Extract the correct \"Download Audio File\" link\n",
    "                    for link in new_soup.find_all(\"a\", class_=\"button download-link\"):\n",
    "                        if \"Download Audio File\" in link.get_text(strip=True):\n",
    "                            href = link.get(\"href\")\n",
    "                            download_attr = link.get(\"download\")\n",
    "                            if href and href.startswith(\"http\") and download_attr:\n",
    "                                audio_download_url = href  # Store updated Audio File link\n",
    "                except:\n",
    "                    print(f\"No valid Audio File link found for: {url}\")\n",
    "\n",
    "            # Extract Metadata Fields\n",
    "            field_data = {}\n",
    "            for field in soup.find_all(class_=\"field\"):\n",
    "                label_element = field.find(class_=\"field__label\")\n",
    "                value_elements = field.find_all(class_=\"field__item\")\n",
    "\n",
    "                if label_element and value_elements:\n",
    "                    field_name = label_element.text.strip()\n",
    "                    field_values = \"; \".join([value.text.strip() for value in value_elements])  # Handle multiple values\n",
    "                    field_data[field_name] = field_values\n",
    "\n",
    "            # Store Data in DataFrame\n",
    "            row_data = row.to_dict()  # Convert DataFrame row to dictionary\n",
    "            row_data.update(field_data)  # Merge extracted field data\n",
    "            row_data[\"pdf_download_url\"] = pdf_download_url  # Add PDF URL\n",
    "            row_data[\"audio_download_url\"] = audio_download_url  # Add Audio URL\n",
    "            \n",
    "            # Add today's date in \"retrieved_date\" (YYYY-MM-DD format)\n",
    "            row_data[\"retrieved_date\"] = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            valid_entries.append(row_data)  # Store valid entry\n",
    "\n",
    "        except requests.Timeout:\n",
    "            print(f\"Request timed out for URL: {url}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error accessing {url}: {e}\")\n",
    "\n",
    "    # Convert valid entries into a new DataFrame\n",
    "    filtered_df = pd.DataFrame(valid_entries)\n",
    "\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL and text to search\n",
    "base_url = \"https://www.jfklibrary.org/asset-viewer/archives/\"\n",
    "search_text = \"Download PDF Transcript\"\n",
    "\n",
    "# Process the DataFrame\n",
    "updated_df = check_for_text_in_links_verbose(subset_df, base_url, search_text, driver)\n",
    "\n",
    "display(updated_df.shape)\n",
    "display(updated_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "metadata_df = updated_df.drop(columns=['href', 'Person(s)'])\n",
    "\n",
    "metadata_df = metadata_df.rename(columns={\n",
    "    \"full_url\": \"item_link\",\n",
    "    \"text\": \"item_link_text\",\n",
    "})\n",
    "\n",
    "metadata_df.columns = metadata_df.columns.str.lower().str.replace(r'[^\\w\\s]', '', regex=True).str.replace(' ', '_')\n",
    "\n",
    "#metadata_df['file_name'] = metadata_df['digital_identifier'].astype(str) + \".pdf\"\n",
    "# move to first col\n",
    "metadata_df.insert(0, 'original_file_name', metadata_df['digital_identifier'].astype(str) + \".pdf\")\n",
    "\n",
    "\n",
    "display(metadata_df.shape)\n",
    "display(metadata_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "metadata_df.to_csv(f\"{data_folder}/raw_data/machine_collected/jfk_library/returned_peace_corps_volunteers/metadata.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDFs\n",
    "def download_pdfs_with_custom_names(df, download_folder, base_url, pdf_text):\n",
    "\n",
    "    \n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)  # Create the folder if it doesn't exist\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        url = row['item_link']\n",
    "        file_name = row['digital_identifier'].replace('/', '_')  # Replace slashes in file name to avoid file system issues\n",
    "        if not file_name.endswith(\".pdf\"):\n",
    "            file_name += \".pdf\"  # Ensure the file has a .pdf extension\n",
    "\n",
    "        try:\n",
    "            #print(f\"Visiting URL: {url}\")\n",
    "            response = requests.get(url, timeout=10)  # Set a timeout\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the download link\n",
    "            pdf_link = None\n",
    "            for a_tag in soup.find_all('a', href=True, text=True):\n",
    "                if pdf_text.lower() in a_tag.get_text(strip=True).lower():\n",
    "                    pdf_link = a_tag['href']\n",
    "                    break\n",
    "\n",
    "            if pdf_link:\n",
    "                # If the PDF link is relative, make it absolute\n",
    "                if not pdf_link.startswith(\"http\"):\n",
    "                    pdf_link = base_url + pdf_link\n",
    "\n",
    "                # Download the PDF\n",
    "                #print(f\"Found PDF link: {pdf_link}\")\n",
    "                pdf_response = requests.get(pdf_link, timeout=10)\n",
    "                pdf_response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "                # Save the PDF with the custom file name\n",
    "                pdf_filepath = os.path.join(download_folder, file_name)\n",
    "                with open(pdf_filepath, 'wb') as pdf_file:\n",
    "                    pdf_file.write(pdf_response.content)\n",
    "                print(f\"Downloaded: {pdf_filepath} from {url}\")\n",
    "            else:\n",
    "                print(f\"No PDF download link found on: {url}\")\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error accessing {url}: {e}\")\n",
    "\n",
    "\n",
    "# Assume `pdf_links_df` contains the filtered DataFrame with `full_url` column\n",
    "download_folder = f\"{data_folder}/raw_data/machine_collected/jfk_library/returned_peace_corps_volunteers\"  # Specify the folder to save PDFs\n",
    "base_url = \"https://www.jfklibrary.org\"  # Update if needed\n",
    "pdf_text = \"Download PDF Transcript\"  # Text to identify the PDF download link\n",
    "\n",
    "download_pdfs_with_custom_names(metadata_df, download_folder, base_url, pdf_text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yield_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
