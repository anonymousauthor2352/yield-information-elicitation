{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Imports ----------------\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import yaml\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Config ----------------\n",
    "with open(\"../../../config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "data_folder = os.path.join(config[\"paths\"][\"proj_store\"], \"data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Setup ----------------\n",
    "resource_choice = 'veterans-oral-histories' #nprc-oral-histories #oral-history-at-the-national-archives #veterans-oral-histories #assembly-oral-histories\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "base_url = f'https://www.archives.gov/about/history/{resource_choice}'\n",
    "\n",
    "# Directory to save downloaded PDFs\n",
    "download_dir = f'{data_folder}/raw_data/machine_collected/nara/{resource_choice.replace(\"-\", \"_\")}'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# File for metadata\n",
    "metadata_file = f'{download_dir}/metadata.csv'\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(url, folder):\n",
    "    local_filename = os.path.join(folder, url.split('/')[-1])\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    print(f'Downloaded: {local_filename}')\n",
    "    return local_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Main ----------------\n",
    "# Fetch the webpage content\n",
    "response = requests.get(base_url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all <p> tags with links to PDFs\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "metadata = []\n",
    "\n",
    "for i, p in enumerate(all_paragraphs):\n",
    "    link = p.find('a', href=lambda href: href and href.lower().endswith('.pdf'))\n",
    "    if link:\n",
    "        pdf_url = urljoin(base_url, link['href'])\n",
    "        pdf_filename = link['href'].split('/')[-1]\n",
    "        name = link.get_text(strip=True)\n",
    "\n",
    "        # Look for the next <p> as the description\n",
    "        description = \"No description\"\n",
    "        if i + 1 < len(all_paragraphs):  # Check if there is a next <p>\n",
    "            next_p = all_paragraphs[i + 1]\n",
    "            if next_p:  # Ensure itâ€™s not empty\n",
    "                # Remove any nested <a> tags from the description\n",
    "                for a_tag in next_p.find_all('a'):\n",
    "                    a_tag.unwrap()  # Remove <a> tags but keep their text\n",
    "                description = next_p.get_text(strip=False)\n",
    "\n",
    "        # Download the PDF\n",
    "        downloaded_path = download_file(pdf_url, download_dir)\n",
    "\n",
    "        # Record metadata\n",
    "        metadata.append({\n",
    "            'interviewee_name': name,\n",
    "            'description': description,\n",
    "            'file_url': pdf_url,\n",
    "            'collection_url': base_url,\n",
    "            'original_file_name': pdf_filename,\n",
    "            'retrieved_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        })\n",
    "\n",
    "# Write metadata to a CSV file\n",
    "with open(metadata_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['interviewee_name', 'description', 'file_url', 'collection_url', 'original_file_name', 'retrieved_date'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(metadata)\n",
    "\n",
    "print(f'Metadata written to {metadata_file}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yield_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
