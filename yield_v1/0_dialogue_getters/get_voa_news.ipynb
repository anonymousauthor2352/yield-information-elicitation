{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Imports ----------------\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Config ----------------\n",
    "with open(\"../../../config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "data_folder = os.path.join(config[\"paths\"][\"proj_store\"], \"data\")\n",
    "\n",
    "\n",
    "# Directory to save HTML files\n",
    "output_dir = f\"{data_folder}/raw_data/machine_collected/voa_news\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Setup ----------------\n",
    "# Define the range of pages to scrape\n",
    "START_PAGE = 1  \n",
    "END_PAGE = 21   \n",
    "\n",
    "BASE_URL = \"https://www.voanews.com/s?k=%22VOA+Interview%22&tab=any-content&pi={}&r=any&pp=50\"\n",
    "\n",
    "\n",
    "\n",
    "def get_articles_from_page(page_number):\n",
    "    url = BASE_URL.format(page_number)\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page {page_number}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = []\n",
    "\n",
    "    # Find all article containers\n",
    "    for article in soup.find_all(\"li\", class_=\"fui-grid__inner\"):\n",
    "        title_element = article.find(\"h4\", class_=\"media-block__title\")\n",
    "        link_element = article.find(\"a\", title=True)  # Works for both article structures\n",
    "        summary_element = article.find(\"p\", class_=\"perex\")\n",
    "        date_element = article.find(\"span\", class_=\"date\")\n",
    "        author_element = article.find(\"a\", class_=\"links__item-link\")\n",
    "\n",
    "        if title_element and link_element:\n",
    "            title = title_element.get_text(strip=True)\n",
    "            link = \"https://www.voanews.com\" + link_element[\"href\"]\n",
    "            summary = summary_element.get_text(strip=True) if summary_element else \"\"\n",
    "            date = date_element.get_text(strip=True) if date_element else \"Unknown Date\"\n",
    "            author = author_element.get_text(strip=True) if author_element else \"Unknown Author\"\n",
    "\n",
    "            articles.append({\n",
    "                \"title\": title,\n",
    "                \"document_link\": link,\n",
    "                \"summary\": summary,\n",
    "                \"date\": date,\n",
    "                \"author\": author\n",
    "            })\n",
    "    \n",
    "    return articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape within the specified page range\n",
    "all_articles = []\n",
    "\n",
    "for page_number in range(START_PAGE, END_PAGE + 1):\n",
    "    print(f\"Fetching page {page_number}...\")\n",
    "    articles = get_articles_from_page(page_number)\n",
    "    \n",
    "    if not articles:\n",
    "        print(f\"No articles found on page {page_number}. Skipping to next.\")\n",
    "        continue\n",
    "\n",
    "    all_articles.extend(articles)\n",
    "    time.sleep(2)  # Pause to avoid rate limiting\n",
    "\n",
    "# Store results\n",
    "df = pd.DataFrame(all_articles)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df[df['title'].str.contains(r'\\bVOA interview\\b', case=False, na=False, regex=True)].reset_index(drop=True)\n",
    "\n",
    "display(df_subset)\n",
    "\n",
    "#df_subset.to_csv(f'{output_dir}/metadada.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_voa_html(row):\n",
    "    url = row['document_link']\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Ensure request was successful\n",
    "\n",
    "        # Extract filename and ensure single .html extension\n",
    "        base_filename = url.split(\"/\")[-1].split(\".html\")[0]  # Remove existing .html if present\n",
    "        original_filename = f\"{base_filename}.html\"\n",
    "        file_path = os.path.join(output_dir, original_filename)\n",
    "\n",
    "        # Save HTML content\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response.text)\n",
    "\n",
    "        print(f\"Saved: {file_path}\")\n",
    "\n",
    "        return pd.Series([original_filename, datetime.now().strftime('%Y-%m-%d')])  # Return filename & retrieval date\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "# Apply function and update metadata\n",
    "df_subset[['original_file_name', 'retrieved_date']] = df_subset.apply(save_voa_html, axis=1)\n",
    "\n",
    "# Display updated DataFrame\n",
    "display(df_subset)\n",
    "\n",
    "# Save updated metadata\n",
    "df_subset.to_csv(f'{output_dir}/metadata.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yield_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
