{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Imports ----------------\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import yaml\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Config ----------------\n",
    "with open(\"../../../config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "data_folder = os.path.join(config[\"paths\"][\"proj_store\"], \"data\")\n",
    "\n",
    "# Specify the directory where files should be saved\n",
    "save_directory = f\"{data_folder}/raw_data/machine_collected/jsc_oral_history/\"  \n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# URL of the participants page\n",
    "url = 'https://historycollection.jsc.nasa.gov/JSCHistoryPortal/history/oral_histories/participants.htm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Setup ----------------\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Dictionary to store names and ensure uniqueness\n",
    "name_category_map = {}\n",
    "\n",
    "# Track the most recent valid bolded text (category)\n",
    "current_category = None\n",
    "after_table = False  # Flag to track when we have passed a </table>\n",
    "first_category_set = False  # Ensures the first category is assigned immediately\n",
    "\n",
    "# Process all elements in order (keeping structure)\n",
    "for element in soup.find_all(['table', 'b', 'strong']):\n",
    "    \n",
    "    # If it's a <b> or <strong> and we haven't assigned the first category, set it immediately\n",
    "    if not first_category_set and element.name in ['b', 'strong']:\n",
    "        current_category = \" \".join(element.get_text(separator=\" \", strip=True).split())\n",
    "        first_category_set = True  # Mark that the first category has been set\n",
    "        continue  # Move to the next element\n",
    "    \n",
    "    # If it's a <table>, process the names inside\n",
    "    if element.name == 'table' and 'role' in element.attrs and element['role'] == 'presentation':\n",
    "        after_table = True  # Mark that we've processed a table\n",
    "        \n",
    "        if not current_category:\n",
    "            continue  # Skip tables if no category has been set yet\n",
    "\n",
    "        for a_tag in element.find_all('a', href=True):\n",
    "            name = a_tag.text.strip()\n",
    "            link = a_tag['href'].strip()\n",
    "\n",
    "            # Ensure the link is valid and prepend base URL if necessary\n",
    "            if link.startswith('/'):\n",
    "                link = 'https://historycollection.jsc.nasa.gov' + link\n",
    "            elif not link.startswith('http'):\n",
    "                link = 'https://historycollection.jsc.nasa.gov/JSCHistoryPortal/history/oral_histories/' + link\n",
    "\n",
    "            # Clean up the name: remove all excessive spaces and newlines\n",
    "            name = \" \".join(name.split())\n",
    "\n",
    "            # Only assign a category if the name hasn't been seen before\n",
    "            if name not in name_category_map:\n",
    "                name_category_map[name] = (current_category, link)\n",
    "\n",
    "    # If we have just passed a </table>, pick the FIRST <b> or <strong> as the new category\n",
    "    elif after_table and element.name in ['b', 'strong']:\n",
    "        category_text = \" \".join(element.get_text(separator=\" \", strip=True).split())  # Remove extra spaces\n",
    "\n",
    "        if category_text:  # Ignore empty or meaningless tags\n",
    "            current_category = category_text  # Set the new category\n",
    "            after_table = False  # Reset flag so we don't overwrite it again\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df = pd.DataFrame(\n",
    "    [(cat, name, link) for name, (cat, link) in name_category_map.items()], \n",
    "    columns=['category', 'targets', 'collection_link']\n",
    ")\n",
    "\n",
    "\n",
    "# Function to reformat names (Last, First -> First Last)\n",
    "def reformat_name(name):\n",
    "    parts = name.split(\", \")\n",
    "    if len(parts) == 2:\n",
    "        return f\"{parts[1]} {parts[0]}\"  # Swap order\n",
    "    return name  # Return as is if format is unexpected\n",
    "\n",
    "df['targets'] = df['targets'].apply(reformat_name)\n",
    "\n",
    "df['targets'] = df['targets'].str.replace(',', '', regex=True)\n",
    "df['targets'] = df['targets'].str.replace('..', '.', regex=False)\n",
    "\n",
    "\n",
    "\n",
    "# Remove rows without a target\n",
    "df = df.dropna(subset=['targets'])  # Removes NaN values\n",
    "df = df[df['targets'].str.strip() != '']  # Removes empty strings\n",
    "\n",
    "\n",
    "# clean categories\n",
    "df['category'] = df['category'].replace('January 31, 2020', 'JSC Oral History Project')\n",
    "df = df[df['category'] != 'Please note: Links on this page are active once the Oral History transcript is archived in the JSC History Collection.']\n",
    "\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_link(row):\n",
    "    base_url = re.sub(r'[^/]+$', '', row['collection_link'])  # Remove the last part of URL\n",
    "    return urljoin(base_url, row['original_file_name'])  # Append new filename\n",
    "\n",
    "\n",
    "def update_dataframe(df):\n",
    "    df['document_link'] = df.apply(generate_full_link, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_html_links(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise error if request fails\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract only anchors where text contains 'HTML' in any casing\n",
    "        links = [a['href'] for a in soup.find_all('a', href=True) if 'html' in a.text.strip().lower()]\n",
    "\n",
    "        return links\n",
    "    except requests.RequestException:\n",
    "        return []  # Return empty list on request failure\n",
    "\n",
    "\n",
    "def process_row(row):\n",
    "    base_url = row['collection_link']\n",
    "    original_file_names = extract_html_links(base_url)\n",
    "\n",
    "    if original_file_names:\n",
    "        return [{**row, 'original_file_name': link} for link in original_file_names]\n",
    "    else:\n",
    "        return [{**row, 'original_file_name': None}]\n",
    "\n",
    "\n",
    "def expand_dataframe(df, url_column):\n",
    "    expanded_data = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=24) as executor:\n",
    "        future_to_row = {executor.submit(process_row, row): row for _, row in df.iterrows()}\n",
    "\n",
    "        for i, future in enumerate(as_completed(future_to_row)):\n",
    "            expanded_data.extend(future.result())\n",
    "\n",
    "            # Print update every 100 iterations\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Processed {i + 1} rows...\")\n",
    "\n",
    "    print(\"Expansion complete.\")\n",
    "    return pd.DataFrame(expanded_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Main ----------------\n",
    "expanded_df = expand_dataframe(df, 'collection_link')\n",
    "\n",
    "# Remove rows where 'original_file_name' is NaN or an empty string\n",
    "expanded_df = expanded_df.dropna(subset=['original_file_name'])  # Removes NaN values\n",
    "expanded_df = expanded_df[expanded_df['original_file_name'].str.strip() != '']  # Removes empty strings\n",
    "\n",
    "# Reset index\n",
    "expanded_df = expanded_df.reset_index(drop=True)\n",
    "\n",
    "# Expand to include full link\n",
    "updated_df = update_dataframe(expanded_df)\n",
    "\n",
    "updated_df['collection'] = \"NASA Johnson Space Center Oral History Project\"\n",
    "\n",
    "# Fix the file name so only the last part of the slash is left\n",
    "updated_df['original_file_name'] = updated_df['original_file_name'].apply(lambda x: os.path.basename(x) if pd.notna(x) else x)\n",
    "\n",
    "# Add the retrieved_date column with the current date\n",
    "updated_df['retrieved_date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Display expanded DataFrame\n",
    "display(updated_df.shape)\n",
    "display(updated_df.head())\n",
    "\n",
    "# Save\n",
    "updated_df.to_csv(f'{save_directory}/metadata.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each URL and save raw HTML\n",
    "for index, row in updated_df.iterrows():\n",
    "    url = row['document_link']\n",
    "    original_filename = row['original_file_name']\n",
    "\n",
    "    # Remove the file extension and ensure a valid filename\n",
    "    base_filename = os.path.splitext(original_filename)[0]\n",
    "    safe_filename = f\"{base_filename}.html\"\n",
    "\n",
    "    try:\n",
    "        # Fetch the raw HTML\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "\n",
    "        # Save raw HTML\n",
    "        file_path = os.path.join(save_directory, safe_filename)\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response.text)\n",
    "\n",
    "        print(f\"Saved raw HTML from {url} to {file_path}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yield_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
