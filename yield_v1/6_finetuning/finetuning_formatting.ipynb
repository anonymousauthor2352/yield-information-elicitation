{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acdf5041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/miniconda3/envs/yield-v1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Imports ----------------\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import yaml\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558dcf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Args ----------------\n",
    "dataset = \"yield-v1-small10pct\"\n",
    "tokenizer_model = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "TOKEN_LIMIT = 512  # max tokens allowed per dialogue\n",
    "WINDOW_SIZE = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc8f94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Config ----------------\n",
    "with open(\"../../../config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "data_path = os.path.join(config[\"paths\"][\"proj_store\"], \"data\")\n",
    "models_folderpath = config[\"paths\"][\"models\"]\n",
    "\n",
    "base_folder = os.path.join(data_path, dataset)\n",
    "output_folder = f\"{base_folder}-finetuning\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Initialize tokenizer globally\n",
    "MODEL_NAME = os.path.join(models_folderpath, tokenizer_model) \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53ff24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dialogue(dialogue, file_path):\n",
    "    turns = dialogue.get(\"turns\", [])\n",
    "    if not turns:\n",
    "        raise ValueError(f\"Dialogue {dialogue.get('dialogue_id')} in {file_path} has no turns.\")\n",
    "\n",
    "    domain = dialogue.get(\"domain\", \"unknown\").replace(\"_\", \" \")\n",
    "    system_prompt = f\"Act as an information elicitation agent for {domain}.\"\n",
    "\n",
    "    merged_blocks = []\n",
    "    block_start_indices = []\n",
    "    current_role = None\n",
    "    utterances = []\n",
    "\n",
    "    index = 0\n",
    "    while index < len(turns) and turns[index][\"role\"] == \"elicitor\":\n",
    "        index += 1\n",
    "\n",
    "    if index >= len(turns):\n",
    "        raise ValueError(f\"Dialogue {dialogue.get('dialogue_id')} in {file_path} has no respondent turn to start with.\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Build merged blocks\n",
    "    for i in range(index, len(turns)):\n",
    "        turn = turns[i]\n",
    "        role = turn[\"role\"]\n",
    "        utterance = turn[\"utterance\"].strip()\n",
    "\n",
    "        if current_role is None:\n",
    "            current_role = role\n",
    "            utterances = [utterance]\n",
    "            start_index = i\n",
    "        else:\n",
    "            if role == current_role:\n",
    "                utterances.append(utterance)\n",
    "            else:\n",
    "                merged_content = f\"{utterances[0]}\"\n",
    "                if len(utterances) > 1:\n",
    "                    merged_content += \"\\n\\n\" + \"\\n\\n\".join(utterances[1:])\n",
    "                merged_blocks.append((current_role, merged_content))\n",
    "                block_start_indices.append(start_index)\n",
    "\n",
    "                current_role = role\n",
    "                utterances = [utterance]\n",
    "                start_index = i\n",
    "\n",
    "    if utterances:\n",
    "        merged_content = f\"{utterances[0]}\"\n",
    "        if len(utterances) > 1:\n",
    "            merged_content += \"\\n\\n\" + \"\\n\\n\".join(utterances[1:])\n",
    "        merged_blocks.append((current_role, merged_content))\n",
    "        block_start_indices.append(start_index)\n",
    "\n",
    "    # SLIDING WINDOW \n",
    "    for window_start in range(len(merged_blocks) - WINDOW_SIZE + 1):\n",
    "        window = merged_blocks[window_start : window_start + WINDOW_SIZE]\n",
    "\n",
    "        # Ensure last block is elicitor â†’ assistant\n",
    "        if window[-1][0] == \"elicitor\":\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "            for role, content in window:\n",
    "                if role == \"elicitor\":\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "                elif role == \"respondent\":\n",
    "                    messages.append({\"role\": \"user\", \"content\": content})\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected role: {role}\")\n",
    "\n",
    "            #full_text = \"\\n\".join([msg[\"content\"] for msg in messages])\n",
    "            \n",
    "            # Format the conversation using the chat template\n",
    "            chat_text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            # Tokenize to count tokens accurately\n",
    "            #print(chat_text) # check output\n",
    "            tokens = tokenizer(chat_text, return_tensors=None, add_special_tokens=False)[\"input_ids\"]\n",
    "            token_count = len(tokens)\n",
    "\n",
    "            if token_count <= TOKEN_LIMIT:\n",
    "                results.append({\n",
    "                    \"block_id\": f\"{dialogue.get('dialogue_id')}:{block_start_indices[window_start]}\",\n",
    "                    \"domain\": dialogue.get('domain'),\n",
    "                    \"messages\": messages\n",
    "                })\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_file(input_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        dialogues = json.load(f)\n",
    "\n",
    "    if not isinstance(dialogues, list):\n",
    "        raise ValueError(f\"Expected list of dialogues in file {input_file}, found {type(dialogues)}\")\n",
    "\n",
    "    processed_dialogues = []\n",
    "\n",
    "    for _, dialogue in enumerate(dialogues):\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            processed_windows = process_dialogue(dialogue, input_file)\n",
    "            for window in processed_windows:\n",
    "                processed_dialogues.append(json.dumps(window, ensure_ascii=False))\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error in {input_file}: {e}\")\n",
    "\n",
    "    return processed_dialogues\n",
    "\n",
    "\n",
    "\n",
    "def process_split(input_folder, output_folder):\n",
    "    json_files = [f for f in os.listdir(input_folder) if f.endswith('.json')]\n",
    "\n",
    "    for json_file in json_files:\n",
    "        file_path = os.path.join(input_folder, json_file)\n",
    "        processed = process_file(file_path)\n",
    "\n",
    "        output_file_path = os.path.join(output_folder, json_file.replace('.json', '.jsonl'))\n",
    "\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f_out:\n",
    "            for line in processed:\n",
    "                f_out.write(line + '\\n')\n",
    "\n",
    "        print(f\"Processed {len(processed)} dialogues from {json_file} into {output_file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca708389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2545 dialogues from train-001.json into /data/yield-v1/data/yield-v1-small10pct-finetuning/train/train-001.jsonl\n",
      "Processed 8725 dialogues from train-000.json into /data/yield-v1/data/yield-v1-small10pct-finetuning/train/train-000.jsonl\n",
      "Processed 2050 dialogues from dev-000.json into /data/yield-v1/data/yield-v1-small10pct-finetuning/dev/dev-000.jsonl\n",
      "Processed 1051 dialogues from test-000.json into /data/yield-v1/data/yield-v1-small10pct-finetuning/test/test-000.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Procedure\n",
    "splits = [\"train\", \"dev\", \"test\"]\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for split in splits:\n",
    "    input_split_folder = os.path.join(base_folder, split)\n",
    "    output_split_folder = os.path.join(output_folder, split)\n",
    "    os.makedirs(output_split_folder, exist_ok=True)\n",
    "\n",
    "    process_split(input_split_folder, output_split_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yield-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
