{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "856e0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Imports ----------------\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import textwrap\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0453d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Args ----------------\n",
    "#model_choice = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "#model_choice = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model_choice = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "adapter_choice = \"20251219T2015-deepseek-r1-distill-llama-8b-sft-m3trained\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714e47bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Config ----------------\n",
    "with open(\"../../config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "data_path = config[\"paths\"][\"proj_store\"]\n",
    "models_folderpath = config[\"paths\"][\"models\"]\n",
    "\n",
    "\n",
    "base_model_path = os.path.join(models_folderpath, model_choice)\n",
    "adapter_model_path = os.path.join(data_path, \"experiments\", \"fine-tuning\", \"adapter-models\", adapter_choice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b62fe0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71f1bc6a54b4881a2d47c2632e9d86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load adapters\n",
    "model = PeftModel.from_pretrained(base_model, adapter_model_path)\n",
    "\n",
    "# Merge adapters into base model\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Inference\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369c981f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Act as an information elicitation agent for journalistic\n",
      "interviews.<｜User｜>Oh, we brainstormed about it. We started, as I said, we did a\n",
      "lot of research on, since our early ages, let's say in all our scientific career\n",
      "is based on embodiment, and activities in these areas and slowly we kept\n",
      "wondering more and more, on the role of these areas. And then, the way it goes,\n",
      "we normally speak about a topic and include other people and keep brainstorming,\n",
      "and I think, that's how it goes. At least for us. And in humans, we are also\n",
      "developing similar paradigms, so it just came one plus one equal to two.\n",
      "[laughs]<｜Assistant｜>What were the roles of other people involved in this\n",
      "study?<｜end▁of▁sentence｜><｜User｜>So, Julen Hernandez-Lallement, the first author\n",
      "of the study, was the one that basically made the vast majority of the\n",
      "experiments. And he was helped by the other two: [Augustine Triumph] Attah: who\n",
      "did part of the other animal groups, and then Cindy [Pinhal] and Efe [Soyman]\n",
      "helped to analyse the data. And basically Valeria and me have been kind of help\n",
      "designed this study initially and helped to analyse the data and wind up the\n",
      "study. So we kind of lead the lab, but it's the other people that really\n",
      "manipulate the rats and do all the hard work.<｜Assistant｜>What was the timeline\n",
      "of this study?<｜end▁of▁sentence｜><｜User｜>Well, it was long.  Yeah, I would say,\n",
      "four years?.<｜Assistant｜>\n"
     ]
    }
   ],
   "source": [
    "# your messages in HF format\n",
    "messages = [{\"role\": \"system\", \"content\": \"Act as an information elicitation agent for journalistic interviews.\"}, {\"role\": \"user\", \"content\": \"Oh, we brainstormed about it. We started, as I said, we did a lot of research on, since our early ages, let's say in all our scientific career is based on embodiment, and activities in these areas and slowly we kept wondering more and more, on the role of these areas. And then, the way it goes, we normally speak about a topic and include other people and keep brainstorming, and I think, that's how it goes. At least for us. And in humans, we are also developing similar paradigms, so it just came one plus one equal to two. [laughs]\"}, {\"role\": \"assistant\", \"content\": \"What were the roles of other people involved in this study?\"}, {\"role\": \"user\", \"content\": \"So, Julen Hernandez-Lallement, the first author of the study, was the one that basically made the vast majority of the experiments. And he was helped by the other two: [Augustine Triumph] Attah: who did part of the other animal groups, and then Cindy [Pinhal] and Efe [Soyman] helped to analyse the data. And basically Valeria and me have been kind of help designed this study initially and helped to analyse the data and wind up the study. So we kind of lead the lab, but it's the other people that really manipulate the rats and do all the hard work.\"}, {\"role\": \"assistant\", \"content\": \"What was the timeline of this study?\"}, {\"role\": \"user\", \"content\": \"Well, it was long.\\n\\nYeah, I would say, four years?.\"}]\n",
    "\n",
    "\n",
    "\n",
    "# get the converted prompt\n",
    "prompt_str = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "prompt_str = prompt_str.replace(\"<think>\", \"\") # in case of deepseek\n",
    "\n",
    "\n",
    "print(textwrap.fill(prompt_str, width=80))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfc7397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Act as an information elicitation agent for journalistic interviews.<｜User｜>Oh, we brainstormed about it. We started, as I said, we did a lot of research on, since our early ages, let's say in all our scientific career is based on embodiment, and activities in these areas and slowly we kept wondering more and more, on the role of these areas. And then, the way it goes, we normally speak about a topic and include other people and keep brainstorming, and I think, that's how it goes. At least for us. And in humans, we are also developing similar paradigms, so it just came one plus one equal to two. [laughs]<｜Assistant｜>What were the roles of other people involved in this study?<｜end▁of▁sentence｜><｜User｜>So, Julen Hernandez-Lallement, the first author of the study, was the one that basically made the vast majority of the experiments. And he was helped by the other two: [Augustine Triumph] Attah: who did part of the other animal groups, and then Cindy [Pinhal] and Efe [Soyman] helped to analyse the data. And basically Valeria and me have been kind of help designed this study initially and helped to analyse the data and wind up the study. So we kind of lead the lab, but it's the other people that really manipulate the rats and do all the hard work.<｜Assistant｜>What was the timeline of this study?<｜end▁of▁sentence｜><｜User｜>Well, it was long.\n",
      "\n",
      "Yeah, I would say, four years?.<｜Assistant｜>\n",
      "What was the reason for this study? What was it that you wanted to study?\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "result = pipe(prompt_str)\n",
    "\n",
    "#print(textwrap.fill(result[0]['generated_text'], width=80))\n",
    "print(result[0]['generated_text'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sequential-ieas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
